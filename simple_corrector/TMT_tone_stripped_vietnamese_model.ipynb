{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flxQCgbQkzTj"
   },
   "source": [
    "## Idea\n",
    "- Giảm thời gian generate data\n",
    "- Train nhiều dữ liệu hơn, dùng GPU\n",
    "- Dùng drop out\n",
    "- Fit dùng validation set cho val_acc\n",
    "- Generate data có dấu\n",
    "    - Giữ dấu/Bỏ bớt dấu/Xóa dấu\n",
    "    - Áp dụng cho cả target character và ngữ cảnh\n",
    "\n",
    "- Sử dụng data chat (câu ngắn, ít chữ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gi3VfaiBbaW0"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import re\n",
    "import string \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QumdrsoGKTLH",
    "outputId": "26bfb620-2081-4aa3-967e-442eefdc23f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus-title.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf /content/drive/MyDrive/chatbot/Binhvq_News_Corpus/corpus-title.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-RDBBXIZbTL",
    "outputId": "1110ee3d-24f7-4a9f-882e-cf300a903a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9487416 corpus-title.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l corpus-title.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtBsOkuBeTpo",
    "outputId": "d42c7bf2-cc4c-487d-a3b8-5ac1673e0fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chây ì nộp phạt nguội.\n",
      "Cháu đòi tiền cơm, dì đòi tiền nhà.\n",
      "Đà Nẵng nghiên cứu tiện ích nhắn tin khi vi phạm đến chủ phương tiện.\n",
      "Khó xử vụ mẹ 70 tuổi trộm xe hơi của con gái.\n",
      "Thay đổi về đăng ký, chuyển nhượng xe từ 12/2 bạn cần biết.\n",
      "Những trường hợp cần trưng cầu giám định trong vụ án kinh tế.\n",
      "Thị trấn Ollolai ở Italia bán nhà với giá hơn 1 USD để thu hút cư dân.\n",
      "Bỏ quy định bán, tặng xe ô tô phải thông báo với công an.\n",
      "Từ 12/2/2018: Người bán, tặng xe ô tô không phải thông báo với công an.\n",
      "Cái kết đắng chát sau 20 năm đám cưới là lời tuyên bố của chồng tệ bạc: 'Giải tán đi, tôi lấy vợ khác'.\n"
     ]
    }
   ],
   "source": [
    "!head corpus-title.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cldew1sVKBvm"
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.build_alphabet()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.alphabet)\n",
    "\n",
    "  def build_alphabet(self):\n",
    "    accented_chars = [\n",
    "      'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n",
    "      'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
    "      'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
    "      'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
    "      'í', 'ì', 'ỉ', 'ĩ', 'ị',\n",
    "      'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n",
    "      'đ',\n",
    "    ]\n",
    "\n",
    "    accented_chars.extend([c.upper() for c in accented_chars])\n",
    "\n",
    "\n",
    "    alphabet = list(chr(0)\n",
    "                    + chr(1)\n",
    "                    + string.printable\n",
    "                    + ''.join(accented_chars))\n",
    "    \n",
    "    self.alphabet = alphabet\n",
    "    self.index_to_char = dict(enumerate(alphabet))\n",
    "    self.char_to_index = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "  def text_to_sequence(self, text):\n",
    "    seq = [self.char_to_index.get(c) for c in text]\n",
    "    seq = [i if i is not None else 1 for i in seq]\n",
    "    return seq\n",
    "\n",
    "  \n",
    "  def sequence_to_text(self, seq):\n",
    "    return [self.index_to_char[i] for i in seq]\n",
    "\n",
    "  def one_hot(self, sequence, one_hot_length):\n",
    "    seq_length = len(sequence)\n",
    "    result = np.zeros((seq_length, one_hot_length))\n",
    "    result[np.arange(seq_length), sequence] = 1\n",
    "    return result\n",
    "\n",
    "  def one_hot_scalar(self, value, one_hot_length):\n",
    "    result  = np.zeros((one_hot_length, ))\n",
    "    result[value] = 1\n",
    "    return result\n",
    "\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtKVW9U1ABSB"
   },
   "outputs": [],
   "source": [
    "class AccentStripper:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.build_accent_stripped_map()\n",
    "\n",
    "  def build_accent_stripped_map(self):\n",
    "    latin_to_accented_char = {\n",
    "      'a': ['á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',],\n",
    "      'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',],\n",
    "      'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',],\n",
    "      'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',],\n",
    "      'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị',],\n",
    "      'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',],\n",
    "      'd': ['đ',]\n",
    "\n",
    "    }\n",
    "    \n",
    "    map = dict()\n",
    "    for k, cs in latin_to_accented_char.items():\n",
    "      for c in cs:\n",
    "        map[c] = k\n",
    "\n",
    "      k = k.upper()\n",
    "      cs = [c.upper() for c in cs]\n",
    "      for c in cs:\n",
    "        map[c] = k\n",
    "\n",
    "    accented_chars = set()\n",
    "    accented_chars.update(map.keys())\n",
    "    accented_chars.update(map.values())\n",
    "\n",
    "    self.accented_char_map = map\n",
    "    self.accented_chars =  accented_chars\n",
    "\n",
    "  def is_target_char(self, c):\n",
    "    return c in self.accented_chars\n",
    "\n",
    "  def strip_accent(self, text):\n",
    "    chars = [c if self.accented_char_map.get(c) is None else self.accented_char_map.get(c) for c in text]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "accent_stripper = AccentStripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNOyGPnQHjP5"
   },
   "outputs": [],
   "source": [
    "CONST_before_sequence_length = 15\n",
    "CONST_after_sequence_length = 15\n",
    "CONST_sequence_length = CONST_before_sequence_length + CONST_after_sequence_length + 1\n",
    "CONST_alphabet_length = len(encoder.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZLCFghcl2oy"
   },
   "outputs": [],
   "source": [
    "def generate_samples(text):\n",
    "  text = text.numpy().decode('utf-8')\n",
    "  padding_before = chr(0) * CONST_before_sequence_length\n",
    "  padding_after = chr(0) * CONST_after_sequence_length\n",
    "\n",
    "  text = padding_before + text + padding_after\n",
    "  stripped_text = accent_stripper.strip_accent(text)\n",
    "  sequence = encoder.text_to_sequence(stripped_text)\n",
    "\n",
    "  xs = []\n",
    "  ys = []\n",
    "\n",
    "  for i, c in enumerate(text):\n",
    "    if not accent_stripper.is_target_char(c):\n",
    "      continue\n",
    "\n",
    "    start = i - CONST_before_sequence_length\n",
    "    end = i + CONST_after_sequence_length + 1\n",
    "\n",
    "    x_sequence = sequence[start:end]\n",
    "    y_sequence = encoder.text_to_sequence(c)[0]\n",
    "\n",
    "    xs.append(x_sequence)\n",
    "    ys.append(y_sequence)\n",
    "\n",
    "  return xs, ys\n",
    "\n",
    "def create_generator(ds_text):\n",
    "  for text in ds_text:\n",
    "    xs, ys = generate_samples(text)\n",
    "    if len(ys) == 0:\n",
    "      continue\n",
    "\n",
    "    xs = tf.constant(xs, dtype=tf.int32)\n",
    "    ys = tf.constant(ys, dtype=tf.int32)\n",
    "    yield xs, ys\n",
    "\n",
    "\n",
    "def create_datasets():\n",
    "  def create_dataset_from_generator(ds_text):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "     lambda: create_generator(ds_text),\n",
    "     output_signature=(\n",
    "         tf.TensorSpec(shape=(None, CONST_sequence_length), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(None, ), dtype=tf.int32)))\n",
    "\n",
    "    ds = ds.unbatch()\n",
    "    ds = ds.batch(256)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "  test_size = int(1e4)\n",
    "  val_size = int(1e3)\n",
    "  train_size = int(1e6)\n",
    "\n",
    "  ds = tf.data.TextLineDataset('/content/corpus-title.txt')\n",
    "  ds = ds.take(test_size)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  ds_test = create_dataset_from_generator(ds)\n",
    "\n",
    "  ds = tf.data.TextLineDataset('/content/corpus-title.txt')\n",
    "  ds = ds.skip(test_size).take(val_size)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  ds_val = create_dataset_from_generator(ds)\n",
    "\n",
    "  ds = tf.data.TextLineDataset('/content/corpus-title.txt')\n",
    "  ds = ds.skip(test_size).skip(val_size).take(train_size)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  ds_train = create_dataset_from_generator(ds)\n",
    "\n",
    "  return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW99k66yEWzn"
   },
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EfQVBOuHOG0",
    "outputId": "aea91db1-9658-4cc4-f887-656709f4922b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 31, 236)           55696     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7316)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1873152   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 236)               60652     \n",
      "=================================================================\n",
      "Total params: 2,055,292\n",
      "Trainable params: 1,999,596\n",
      "Non-trainable params: 55,696\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(input_dim=CONST_alphabet_length,\n",
    "                   output_dim=CONST_alphabet_length, \n",
    "                   embeddings_initializer=tf.keras.initializers.Constant(np.eye(CONST_alphabet_length)),\n",
    "                   input_length = CONST_sequence_length,\n",
    "                   trainable=False),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(CONST_alphabet_length, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m760rdG-NQHi"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kG2L-XJsPrNT",
    "outputId": "c43903fb-4d06-4437-f631-026d86368ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "75787/75787 [==============================] - 726s 10ms/step - loss: 0.3285 - accuracy: 0.8978 - val_loss: 0.3054 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.90505, saving model to /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese/assets\n",
      "Epoch 2/3\n",
      "75787/75787 [==============================] - 726s 10ms/step - loss: 0.2395 - accuracy: 0.9258 - val_loss: 0.2792 - val_accuracy: 0.9146\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.90505 to 0.91455, saving model to /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese/assets\n",
      "Epoch 3/3\n",
      "75787/75787 [==============================] - 731s 10ms/step - loss: 0.2207 - accuracy: 0.9318 - val_loss: 0.2661 - val_accuracy: 0.9177\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.91455 to 0.91767, saving model to /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c30082c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train, ds_val, ds_test = create_datasets()\n",
    "\n",
    "check_point_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese', \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=3, callbacks=[early_stopping_cb, check_point_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbbMZ5byNkiv",
    "outputId": "9e42f7b6-07c6-46f7-a18d-34ed02a87d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757/757 [==============================] - 10s 14ms/step - loss: 0.2662 - accuracy: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26619675755500793, 0.9178197383880615]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(ds_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i98nZ3X5TRMD"
   },
   "source": [
    "## Test a specific sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6QkeVS6TQfb"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('/content/drive/MyDrive/chatbot/TMT_accent_stripped_vietnamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLEJt0Yqep6s",
    "outputId": "00d435d4-9b22-4df7-98ec-92e01a6fc37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \"Cô gái đưa tin lên Facebook đó là phóng viên, đúng ra cô này cũng thuộc nhóm đối tượng ưu tiên trong các nhóm theo quy định của Bộ Y tế.\n",
      "Stripped: \"Co gai dua tin len Facebook do la phong vien, dung ra co nay cung thuoc nhom doi tuong uu tien trong cac nhom theo quy dinh cua Bo Y te.\n",
      "Toned:    \"Cô gái đưa tin lên Facebook đo là phóng viên, đùng ra có này cũng thuốc nhóm đối tượng ưu tiên trong các nhóm theo quy định của Bộ Y tế.\n"
     ]
    }
   ],
   "source": [
    "text = '\"Cô gái đưa tin lên Facebook đó là phóng viên, đúng ra cô này cũng thuộc nhóm đối tượng ưu tiên trong các nhóm theo quy định của Bộ Y tế.'\n",
    "print(f'Original: {text}')\n",
    "\n",
    "text = unicodedata.normalize('NFC', text)\n",
    "text = text.strip()\n",
    "text = accent_stripper.strip_accent(text)\n",
    "print(f'Stripped: {text}')\n",
    "\n",
    "xs, _ = generate_samples(tf.constant(text))\n",
    "ys_pred = model.predict(tf.constant(xs))\n",
    "ys_pred = np.argmax(ys_pred, axis=-1)\n",
    "ys_pred = encoder.sequence_to_text(ys_pred) \n",
    "\n",
    "accented_text = []\n",
    "i = 0\n",
    "for c in text:\n",
    "  if accent_stripper.is_target_char(c):    \n",
    "     accented_text.append(ys_pred[i])\n",
    "     i += 1\n",
    "  else:\n",
    "    accented_text.append(c)\n",
    "\n",
    "accented_text = ''.join(accented_text)\n",
    "print(f'Toned:    {accented_text}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TMT_tone_stripped_vietnamese_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
